# Newsfeed

*A FastAPIâ€‘based newsâ€‘aggregation service for corporate IT managers.*
*It features modular ingestion, relevance filtering, and a ****relevanceÂ Ã—Â recency**** scoring pipeline.*

---

## ğŸš€ Installation

```bash
git clone https://github.com/UmbertoTomasini/newsfeed.git
python3 -m venv newsfeed/venv
source newsfeed/venv/bin/activate
pip install -r newsfeed/requirements.txt
```

> **Tip**â€‚You never `cd` inside the repo in these commands, so everything works from the directory where you ran the clone.

---

## â–¶ï¸ Quick start (local)

> The pipeline first **aggregates IT news** from multiple sources,
> **filters** items relevant to IT managers, then **continuously** fetches new items and rescores them in the background.
> See [ArchitectureÂ &Â design](#architecture--design) for the rationale.

### 0â€‚Activate the virtualâ€‘env (if not already)

```bash
source newsfeed/venv/bin/activate
```

### 1â€‚Start the API server

```bash
uvicorn newsfeed.main:app --reload          # âœ http://127.0.0.1:8000
# Ctrlâ€‘C to stop
```

Open [**http://127.0.0.1:8000/docs**](http://127.0.0.1:8000/docs) for Swagger UI.

### 2â€‚Terminal UIÂ â€“ read the feed

```bash
python -m newsfeed.show_news
```

Items are sorted by **relevanceÂ Ã—Â recency**.

### 3â€‚Synthetic ingestionÂ & retrieval (REST)

```bash
# ingest one synthetic item
curl -X POST http://127.0.0.1:8000/ingest \
  -H 'Content-Type: application/json' \
  -d '[{"id":"test-1","source":"synthetic","title":"Test Event",\
        "body":"Synthetic test event.","published_at":"2024-07-15T10:00:00Z"}]'

# fetch what the filter accepted
curl http://127.0.0.1:8000/retrieve | jq .
```

| Verb   | Path            | Purpose                                                  |
| ------ | --------------- | -------------------------------------------------------- |
| `POST` | `/ingest`       | Push raw items (array)                                   |
| `GET`  | `/retrieve`     | Return *accepted* items, sorted by relevance Ã— recency   |
| `GET`  | `/retrieve-all` | Debug: accepted + rejected + evaluation from a large LLM |

---

## ğŸ—ï¸ Architecture & design

```mermaid
flowchart TD
    src["Aggregation<br/>(Reddit Sysadmin, Reddit Outages, Reddit Cybersec, Arsâ€‘Technica, Mock)"] --> mgr["IngestionÂ Manager"]
    mgr --> hf["HardÂ Filter<br/>BARTâ€‘MNLI (zeroâ€‘shot)"]
    hf --> rec["RecencyÂ Weighting<br/>exp(-Î”t / PERSISTENCE_TIME)"]
    rec --> mem["Inâ€‘memory Store<br/>(â‰¤Â MAX_ITEMS)"]
    subgraph Background
        timer["Background Task Manager<br/>runs every INTERVAL s"] --> mgr
    end
    mem --> api["FastAPI Endpoints"]
    api --> ui["Terminal UI / REST client"]
```

| Stage                       | What happens                                                                                                                                                                                                                                                                                                                                               | Key **config.py** knobs                             |
| --------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------- |
| **Aggregation**             | Each source fetches `NUMBER_INITIAL_POST_PER_SOURCE` items and normalises them into `NewsItem` objects (`id`, `source`, `title`, `body`, `published_at`, â€¦).                                                                                                                                                                                               | `NUMBER_INITIAL_POST_PER_SOURCE`, `INTERVAL`        |
| **Ingestion manager**       | Deduplicates on `id`, stamps metadata, pushes batch to filter.                                                                                                                                                                                                                                                                                             | â€”                                                   |
| **Hard filter**             | `facebook/bartâ€‘largeâ€‘mnli` zeroâ€‘shot classifier checks *titleÂ + first 2 sentences* against a specialised label set (see below). Item is accepted if **any** label score â‰¥ `MIN_SCORE`. Rejected items are stored *only* when `ASSESS_CORRECTNESS_WITH_BIGGER_MODEL=True`.                                                                                  | `MIN_SCORE`, label list in `ingestion/filtering.py` |
| **Recency weighting**       | Compute `recency_weight = exp(-Î”t / PERSISTENCE_TIME)` and save `final_score = relevance_score Ã— recency_weight`.                                                                                                                                                                                                                                          | `PERSISTENCE_TIME`                                  |
| **In-memory store**         | Keep accepted items up to `MAX_ITEMS`; older items drop off.                                                                                                                                                                                                                                                                                               | `MAX_ITEMS`                                         |
| **Background Task Manager** | Async loop fetches new items every `INTERVAL` seconds, reâ€‘applies the filterÂ & scoring.                                                                                                                                                                                                                                                                    | `INTERVAL`                                          |
| **API / UI**                | FastAPI exposes `/ingest`, `/retrieve`, `/retrieve-all`; by default the TUI (`python -m newsfeed.show_news`) calls `/retrieve`, which recomputes recency weights. If `ASSESS_CORRECTNESS_WITH_BIGGER_MODEL=True`, the TUI instead calls `/retrieve-all`, receives *all* items (acceptedÂ +Â rejected) and then performs live evaluation with a larger model. | â€”                                                   |

> **Configuration note**â€‚All parameters in **CAPS** above live in [`newsfeed/config.py`](newsfeed/config.py). Edit them there to change thresholds, intervals, or featureâ€‘flags.

### Classifier label set & decision rule

| Bucket           | Labels                                                                                                  |
| ---------------- | ------------------------------------------------------------------------------------------------------- |
| **Relevant**     | `Outage`, `SecurityÂ Incident`, `Vulnerability`, `MajorÂ Bug`, *(plus 8Â more nuanced operational labels)* |
| **Not relevant** | `Not a critical/urgent issue for an IT manager of a company`                                            |

* The label list is intentionally **skewed toward relevant classes** to minimise falseâ€‘negatives, which matter more than falseâ€‘positives in this setting.
* The classifier runs in **multiâ€‘label** mode (`multi_label=True`). If *any* labelâ€™s probability â‰¥ `MIN_SCORE`, the item is accepted; otherwise rejected.
* **RecallÂ / latency tradeâ€‘off** â€“ more labels boost recall but increase inference latency. The current list was chosen as the sweetâ€‘spot observed in benchmarking.
* For every processed item we store:

  1. `relevance_score` â†’ the *max* label probability.
  2. `top_relevant_label` â†’ the label that produced that score.

Rejected items are only persisted when `ASSESS_CORRECTNESS_WITH_BIGGER_MODEL=True` so that a larger model can later reassess them for offline metrics.

---

\*\*â€‚All parameters in **CAPS** above live in [`newsfeed/config.py`](newsfeed/config.py). Edit them there to change thresholds, intervals, or featureâ€‘flags.

---

## ğŸ§ª Testing & verification

| Level       | Whatâ€™s covered                                                            | How to run                      |
| ----------- | ------------------------------------------------------------------------- | ------------------------------- |
| Unit        | Ingestion adapters, `filtering.score()`, recency decay                    | `pytest tests/unit -q`          |
| Integration | Endâ€‘toâ€‘end pipeline with mock sources â†’ `/retrieve`                       | `pytest tests/integration -q`   |
| Performance | LatencyÂ / throughput logged via `log_utils` when `ASSESS_EFFICIENCY=True` | Inspect `logs/efficiency/*.log` |

The CI workflow (`.github/workflows/ci.yml`) runs **pytest** on Pythonâ€¯3.10â€¯&â€¯3.11 and enforces code health with **BlackÂ +Â isortÂ +Â Ruff**.

---

## ğŸ‘“ Evaluation of efficiency & correctness&#x20;

### Correctness

1. **Offline metrics**Â â€“ precision, recall, and a full confusionâ€‘matrix are computed on a customâ€‘labelled dataset (`tests/test_hard_filtering_relevant.py`).
2. **Live evaluation**Â â€“ run `python -m newsfeed.show_news` with `ASSESS_CORRECTNESS_WITH_BIGGER_MODEL=True` to stream items through a larger LLM and compare its classification to the pipeline in real time.

### Efficiency

* Latency, throughput, CPU and (if present) GPU usage are measured **per pipeline step**.
* Results are appended to timestamped files under `logs/efficiency/` when `ASSESS_EFFICIENCY=True` in `config.py`.

---

## âš™ï¸ Configuration

| Variable                               | Description                        | Default |
| -------------------------------------- | ---------------------------------- | ------- |
| `MIN_SCORE`                            | Minimum relevance score to accept  | `0.08`  |
| `MAX_ITEMS`                            | Max items kept in memory           | `100`   |
| `INTERVAL`                             | Ingestion interval (s)             | `30`    |
| `NUMBER_INITIAL_POST_PER_SOURCE`       | Seed items per source              | `5`     |
| `PERSISTENCE_TIME`                     | Recency decay constant (s)         | `86400` |
| `ASSESS_CORRECTNESS_WITH_BIGGER_MODEL` | Run offline eval with larger model | False   |
| `ASSESS_EFFICIENCY`                    | Log latency & throughput           | `True`  |

See [`newsfeed/config.py`](newsfeed/config.py) for full commentary.

---

## ğŸ“„ License

MIT
